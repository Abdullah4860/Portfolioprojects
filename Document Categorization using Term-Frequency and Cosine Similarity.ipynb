{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gPXUJvnITR7",
        "outputId": "1e7079c7-d2ec-4a80-be75-523449cd2e39"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n4XPYwJr5fu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import fitz  \n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY5nQ9RlIZJd"
      },
      "outputs": [],
      "source": [
        "# Function to read text from a PDF file\n",
        "def read_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = ''\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc[page_num]\n",
        "        text += page.get_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0kGbvLMr7oG",
        "outputId": "d16abf59-cf16-4c90-c7f6-d838444dd04d"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the papers folder\n",
        "papers_path = '/content/drive/MyDrive/papers/'\n",
        "\n",
        "\n",
        "\n",
        "# Function to read documents from a folder\n",
        "def read_documents(folder_path):\n",
        "    documents = []\n",
        "    file_names = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if file_path.endswith('.pdf'):\n",
        "            content = read_pdf(file_path)\n",
        "            documents.append(content)\n",
        "            file_names.append(filename)\n",
        "\n",
        "    return documents, file_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AETMZIResANs"
      },
      "outputs": [],
      "source": [
        "# Function to construct a DataFrame representing the term-document matrix\n",
        "def create_term_doc_matrix(documents, file_names):\n",
        "    # Step 1: Gather unique terms from all documents\n",
        "    vocabulary = set()\n",
        "    for document in documents:\n",
        "        terms = clean_and_split_document(document)  # Assumes a function for preprocessing\n",
        "        vocabulary.update(terms)\n",
        "\n",
        "    # Print vocabulary for inspection\n",
        "    print(\"vocab\", vocabulary)\n",
        "\n",
        "    # Step 2: Initialize an empty DataFrame\n",
        "    matrix = pd.DataFrame(0, index=list(vocabulary), columns=file_names)\n",
        "\n",
        "    # Step 3: Fill the matrix with term frequencies\n",
        "    for i, document in enumerate(documents):\n",
        "        terms = clean_and_split_document(document)  # Preprocess the document\n",
        "        for term in terms:\n",
        "            matrix.at[term, file_names[i]] += 1  # Increment the count for each term in the document\n",
        "\n",
        "    # Display the resulting matrix\n",
        "    print(matrix)\n",
        "    return matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdbO4Pl_sHUI",
        "outputId": "22d9dcac-6fcb-43da-abe6-d15894d3480d"
      },
      "outputs": [],
      "source": [
        "# Initialization for document organization:\n",
        "# - all_documents: Stores the content of every document\n",
        "# - all_file_names: Stores the names of all documents\n",
        "# - doc_category_lookup: Maps file names to their respective categories\n",
        "all_documents = []\n",
        "all_file_names = []\n",
        "doc_category_lookup = {}\n",
        "\n",
        "# Define the categories to be processed\n",
        "categories = ['NLP', 'CV', 'Cryptography', 'Virtual Reality', 'Internet of Things']\n",
        "\n",
        "# Process each category:\n",
        "for category in categories:\n",
        "    # Construct the path to the category's documents\n",
        "    category_path = os.path.join(papers_path, category)  # Combine base path with category name\n",
        "\n",
        "    # Read documents and file names from the current category's path\n",
        "    documents, file_names = read_documents(category_path)  # Assumes a function for reading documents\n",
        "\n",
        "    # Add category-filename pairs to the lookup dictionary\n",
        "    for filename in file_names:\n",
        "        doc_category_lookup[filename] = category\n",
        "\n",
        "    # Combine documents and file names from all categories into master lists\n",
        "    all_documents.extend(documents)  # Add documents to the master list\n",
        "    all_file_names.extend(file_names)  # Add file names to the master list\n",
        "\n",
        "# Create a term-document matrix to represent word frequencies across documents\n",
        "term_doc_matrix = create_term_doc_matrix(all_documents, all_file_names)  # Assumes a function for matrix creation\n",
        "\n",
        "# Normalize the matrix using logarithmic scaling to balance term frequencies\n",
        "term_doc_matrix_log = np.log1p(term_doc_matrix)  # Add 1 before log to handle zeros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmfB1LSGJc1l",
        "outputId": "83121540-b74f-4f1c-8fdd-e0d55fbff7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'MULTILINGUALNLP.pdf': 'NLP', 'ReinforcementLearningbasedNLP.pdf': 'NLP', 'GOPI.NLP_TERM.pdf': 'NLP', 'Article4-NLP-edit.pdf': 'NLP', 'Aslam2592023JERR107753NLP.pdf': 'NLP', 'Reviewpaper.pdf': 'NLP', '45.LeveragingNLPandMachineLearningforDataIntegrityinClinicalTrials.pdf': 'NLP', 'Machine_Learning_Advancements_in_SQL_Injection_Det.pdf': 'NLP', 'Combining_computer_vision_and_standardised_protoco.pdf': 'CV', 'Leaping_into_the_future_Current_application_and_fu.pdf': 'CV', 'Computer-visionbasedanalysisoftheneurosurgicalsceneAsystematicreview.pdf': 'CV', 'Virtual_reality_experiences_in_medical_education_A.pdf': 'Virtual Reality', 'Legal_aspects_of_functional_security_standardisati.pdf': 'Internet of Things'}\n"
          ]
        }
      ],
      "source": [
        "print(doc_category_lookup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGdUic4WscPN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Function to find the most similar document category for a test document\n",
        "def find_most_similar_category(test_document, term_doc_matrix_log):\n",
        "    global doc_category_lookup  # Access the global dictionary for category lookup\n",
        "\n",
        "    # Gather unique terms from the term-document matrix\n",
        "    terms = set(term_doc_matrix_log.index)\n",
        "\n",
        "    # Create a term frequency vector for the test document, counting whole-word occurrences\n",
        "    test_vector = []\n",
        "    for term in terms:\n",
        "        count = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(term), test_document))\n",
        "        test_vector.append(count)\n",
        "\n",
        "    # Print vector length for debugging\n",
        "    print(len(test_vector))\n",
        "\n",
        "    # Normalize the test vector using logarithmic scaling (add 1 before log to handle zeros)\n",
        "    test_vector = np.log1p(test_vector)\n",
        "\n",
        "    # Calculate cosine similarity with each document in the term-document matrix\n",
        "    similarities = []\n",
        "    for col in term_doc_matrix_log.columns:\n",
        "        col_vector = term_doc_matrix_log[col].values  # Get vector for the current document\n",
        "        similarity = np.dot(test_vector, col_vector) / (np.linalg.norm(test_vector) * np.linalg.norm(col_vector))  # Cosine similarity\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    # Print similarities for debugging\n",
        "    print(\"Similarities:\", similarities)\n",
        "\n",
        "    # Find the index of the most similar document\n",
        "    most_similar_category_index = np.argmax(similarities)\n",
        "\n",
        "    # Print index for debugging\n",
        "    print(\"Most Similar Category Index:\", most_similar_category_index)\n",
        "\n",
        "    # Retrieve the category of the most similar document\n",
        "    if most_similar_category_index < len(term_doc_matrix_log.columns):\n",
        "        most_similar_doc = term_doc_matrix_log.columns[most_similar_category_index]  # Get document name\n",
        "        # Print most similar document for debugging\n",
        "        print(most_similar_doc)\n",
        "        return doc_category_lookup[most_similar_doc]  # Return its category\n",
        "    else:\n",
        "        return \"Unknown Category\"  # Handle cases where no match is found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7YlJh8IpP7f",
        "outputId": "c4344436-6ede-4e81-e550-bff2e21abe5e"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_document_path = '/content/drive/MyDrive/papers/Test/VeterinaryDermatology-2023-Smith-Computervisionmodelforthedetectionofcaninepododermatitisandneoplasiaof.pdf'\n",
        "test_document_content = read_pdf(test_document_path)\n",
        "\n",
        "predicted_category = find_most_similar_category(test_document_content, term_doc_matrix_log)\n",
        "print(f'category: {predicted_category}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL_OyKw6I0Af",
        "outputId": "5726d1d9-4512-41da-933f-80c6491369a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_document_path = '/content/drive/MyDrive/papers/Test/Clickbait_Post_Detection_using_NLP_for_Sustainable.pdf'\n",
        "test_document_content = read_pdf(test_document_path)\n",
        "\n",
        "predicted_category = find_most_similar_category(test_document_content, term_doc_matrix_log)\n",
        "print(f'category: {predicted_category}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
